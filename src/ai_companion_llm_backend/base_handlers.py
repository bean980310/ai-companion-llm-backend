from abc import ABC, abstractmethod
from functools import partial
from typing import Any, BinaryIO
import os
import platform
import warnings
import base64
import random

from typing_extensions import Buffer
import torch.nn
from PIL import Image, ImageFile

try:
    import mlx.nn
except ImportError:
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        warnings.warn("langchain_mlx is not installed. Please install it to use MLX features.", UserWarning)
    else:
        pass
    
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast, PreTrainedTokenizerBase, GenerationMixin, PreTrainedModel, AutoModelForImageTextToText, AutoModel, AutoProcessor, ProcessorMixin, AutoConfig, PretrainedConfig, GenerationConfig
from peft import PeftModel
from llama_cpp import Llama

try:
    from mlx_lm.tokenizer_utils import TokenizerWrapper, SPMStreamingDetokenizer, BPEStreamingDetokenizer, NaiveStreamingDetokenizer
except ImportError:
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        warnings.warn("langchain_mlx is not installed. Please install it to use MLX features.", UserWarning)
    else:
        pass

class BaseModel(ABC):
    def __init__(self, use_langchain: bool = True, **kwargs):
        self.use_langchain = use_langchain
        self.enable_streaming = bool(kwargs.get("enable_streaming", False))

        self.max_tokens = int(kwargs.get("max_tokens", 4096))
        self.max_length = int(kwargs.get("max_length", -1))
        self.seed = int(kwargs.get("seed", 42))
        self.temperature = float(kwargs.get("temperature", 1.0))
        self.top_k = int(kwargs.get("top_k", 50))
        self.top_p = float(kwargs.get("top_p", 1.0))
        self.repetition_penalty = float(kwargs.get("repetition_penalty", 1.0))
        self.enable_thinking = bool(kwargs.get("enable_thinking", False))

        self.langchain_integrator = None

        self.use_chunking = bool(kwargs.get("use_chunking", False))
        self.chunk_size = int(kwargs.get("chunk_size", 1024))

        if self.seed == -1:
            self.seed = random.randint(0, 4294967295)

    @abstractmethod
    def load_model(self):
        pass

    @abstractmethod
    def generate_answer(self, history, **kwargs):
        pass

class BaseModelHandler(BaseModel):
    def __init__(self, model_id: str, lora_model_id: str | None = None, use_langchain: bool = True, image_input: str | Image.Image | ImageFile.ImageFile | Any | None = None, **kwargs):
        super().__init__(use_langchain, **kwargs)
        self.model_id: str = model_id
        self.lora_model_id: str | None = lora_model_id
        self.config: AutoConfig | PretrainedConfig | GenerationConfig | Any | None = None
        self.local_model_path: str = os.path.join("./models/llm", model_id)
        self.local_lora_model_path: str | None = os.path.join("./models/llm/loras", lora_model_id) if lora_model_id else None
        self.image_input = image_input

        self.processor: AutoProcessor | ProcessorMixin | Any | None = None
        self.tokenizer: AutoTokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | PreTrainedTokenizerBase | TokenizerWrapper | type[SPMStreamingDetokenizer] | partial[SPMStreamingDetokenizer] | type[BPEStreamingDetokenizer] | type[NaiveStreamingDetokenizer] | Any | None = None
        self.model: torch.nn.Module | mlx.nn.Module | PreTrainedModel | GenerationMixin | AutoModelForCausalLM | AutoModelForImageTextToText | AutoModel | PeftModel | Llama | Any | None = None

    @abstractmethod
    def load_model(self):
        pass
    
    @abstractmethod
    def generate_answer(self, history, **kwargs):
        pass
    
    @abstractmethod
    def get_settings(self):
        pass
    
    @abstractmethod
    def load_template(self, messages):
        pass

class BaseCausalModelHandler(BaseModelHandler):
    def __init__(self, model_id: str, lora_model_id: str | None = None, use_langchain: bool = True, **kwargs):
        super().__init__(model_id, lora_model_id, use_langchain, **kwargs)
        
    @abstractmethod
    def load_model(self):
        pass
    
    @abstractmethod
    def generate_answer(self, history, **kwargs):
        pass
    
    @abstractmethod
    def get_settings(self):
        pass
    
    @abstractmethod
    def load_template(self, messages):
        pass
class BaseVisionModelHandler(BaseModelHandler):
    def __init__(self, model_id: str, lora_model_id: str | None = None, use_langchain: bool = True, image_input: str | Image.Image | ImageFile.ImageFile | Any | None = None, **kwargs):
        super().__init__(model_id, lora_model_id, use_langchain, image_input, **kwargs)
        
    @abstractmethod
    def load_model(self):
        pass
    
    @abstractmethod
    def generate_answer(self, history, **kwargs):
        pass
    
    @abstractmethod
    def get_settings(self):
        pass
    
    @abstractmethod
    def load_template(self, messages):
        pass
class BaseAPIClientWrapper(BaseModel):
    def __init__(self, selected_model: str, api_key: str | None = None , use_langchain: bool = True, image_input: str | Image.Image | ImageFile.ImageFile | BinaryIO | Buffer | os.PathLike[str] | Any | None = None, **kwargs):
        super().__init__(use_langchain, **kwargs)
        self.model = selected_model
        self.api_key = api_key
        self.image_input = image_input


    @abstractmethod
    def load_model(self):
        pass

    @abstractmethod
    def generate_answer(self, history, **kwargs):
        pass

    @staticmethod
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")